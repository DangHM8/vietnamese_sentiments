import streamlit as st
import torch
import pickle
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from underthesea import word_tokenize

# --- 1. C·∫§U H√åNH GIAO DI·ªÜN ---
st.set_page_config(page_title="Sentiment Analysis Dashboard", layout="centered")

st.title("üìä Ph√¢n lo·∫°i S·∫Øc th√°i C·∫£m x√∫c")
st.markdown("---")

# --- 2. H√ÄM T·∫¢I M√î H√åNH (S·ª≠ d·ª•ng Cache) ---
@st.cache_resource
def load_all_models():
    # 1. T·∫£i PhoBERT t·ª´ Hugging Face
    pb_tokenizer = AutoTokenizer.from_pretrained("danghm/vietnamese_sentiments")
    pb_model = AutoModelForSequenceClassification.from_pretrained("danghm/vietnamese_sentiments")
    
    # 2. T·∫£i Logistic Regression v√† S·ª¨A L·ªñI VERSION
    with open("tfidf_logistic_model.pkl", "rb") as f:
        log_data = pickle.load(f)
        # TH√äM D√íNG N√ÄY ƒê·ªÇ S·ª¨A L·ªñI AttributeError
        if not hasattr(log_data['classifier'], 'multi_class'):
            log_data['classifier'].multi_class = 'auto' 
        
    # 3. T·∫£i Linear SVM v√† S·ª¨A L·ªñI T∆Ø∆†NG T·ª∞ (n·∫øu c√≥)
    with open("svm_sentiment_model.pkl", "rb") as f:
        svm_data = pickle.load(f)
        if not hasattr(svm_data['classifier'], 'multi_class'):
            svm_data['classifier'].multi_class = 'auto'
            
    return pb_tokenizer, pb_model, log_data, svm_data

# Th·ª±c hi·ªán t·∫£i m√¥ h√¨nh
try:
    pb_tokenizer, pb_model, log_data, svm_data = load_all_models()
    label_map = {0: "Ti√™u c·ª±c üò°", 1: "Trung t√≠nh üòê", 2: "T√≠ch c·ª±c üòç"}
except Exception as e:
    st.error(f"‚ö†Ô∏è L·ªói n·∫°p file: {e}. ƒê·∫£m b·∫£o c√°c file .pkl n·∫±m c√πng c·∫•p v·ªõi app.py v√† c√≥ k·∫øt n·ªëi internet ƒë·ªÉ t·∫£i PhoBERT.")
    st.stop()

# --- 3. PH·∫¶N L·ª∞A CH·ªåN M√î H√åNH (Hi·ªÉn th·ªã ngay t·∫°i m√†n h√¨nh ch√≠nh) ---
st.subheader("1. C√†i ƒë·∫∑t c·∫•u h√¨nh")
model_choice = st.selectbox(
    "Ch·ªçn thu·∫≠t to√°n b·∫°n mu·ªën s·ª≠ d·ª•ng ƒë·ªÉ d·ª± ƒëo√°n:",
    ("PhoBERT (Deep Learning)", "Logistic Regression (TF-IDF)", "Linear SVM (TF-IDF)")
)

# Hi·ªÉn th·ªã ghi ch√∫ nhanh v·ªÅ hi·ªáu nƒÉng th·ª±c t·∫ø c·ªßa m√¥ h√¨nh ƒë√£ ch·ªçn
if model_choice == "PhoBERT (Deep Learning)":
    st.success("‚ú® **PhoBERT:** Hi·ªÉu ng·ªØ c·∫£nh t·ªët nh·∫•t. F1-Macro ƒë·∫°t **0.6663** t·∫°i Epoch 2.")
elif model_choice == "Logistic Regression (TF-IDF)":
    st.info("üìà **Logistic:** C√¢n b·∫±ng t·ªët. Accuracy **78.18%** v√† F1-Macro **0.64**.")
else:
    st.warning("‚öñÔ∏è **SVM:** Accuracy cao nh·∫•t (**78.63%**) nh∆∞ng nh·∫≠n di·ªán l·ªõp Trung t√≠nh k√©m (Recall **0.23**).")

st.markdown("---")

# --- 4. KHU V·ª∞C NH·∫¨P D·ªÆ LI·ªÜU & D·ª∞ ƒêO√ÅN ---
st.subheader("2. Nh·∫≠p n·ªôi dung")
user_input = st.text_area("Nh·∫≠p b√¨nh lu·∫≠n kh√°ch h√†ng t·∫°i ƒë√¢y:", height=100, placeholder="V√≠ d·ª•: Shop ph·ª•c v·ª• r·∫•t t·ªët, m√¨nh s·∫Ω ·ªßng h·ªô ti·∫øp...")

if st.button("üîç Ph√¢n t√≠ch c·∫£m x√∫c"):
    if not user_input.strip():
        st.error("Vui l√≤ng kh√¥ng ƒë·ªÉ tr·ªëng √¥ nh·∫≠p li·ªáu!")
    else:
        # Ti·ªÅn x·ª≠ l√Ω chung (T√°ch t·ª´ ti·∫øng Vi·ªát)
        text_segmented = word_tokenize(user_input, format="text")
        
        # Bi·∫øn trung gian ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£
        final_label = ""
        final_conf = None

        # Logic d·ª± ƒëo√°n theo t·ª´ng m√¥ h√¨nh
        if model_choice == "PhoBERT (Deep Learning)":
            inputs = pb_tokenizer(text_segmented, return_tensors="pt", padding=True, truncation=True, max_length=128)
            with torch.no_grad():
                outputs = pb_model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).numpy()[0]
                idx = np.argmax(probs)
                final_label = label_map[idx]
                final_conf = probs[idx]

        elif model_choice == "Logistic Regression (TF-IDF)":
            tfidf_vec = log_data['vectorizer']
            clf = log_data['classifier']
            X_tfidf = tfidf_vec.transform([text_segmented])
            idx = clf.predict(X_tfidf)[0]
            final_label = label_map[idx]
            final_conf = clf.predict_proba(X_tfidf).max()

        else: # Linear SVM
            tfidf_vec = svm_data['vectorizer']
            clf = svm_data['classifier']
            X_tfidf = tfidf_vec.transform([text_segmented])
            idx = clf.predict(X_tfidf)[0]
            final_label = label_map[idx]
            # SVM kh√¥ng h·ªó tr·ª£ ƒë·ªô tin c·∫≠y m·∫∑c ƒë·ªãnh

        # --- 5. HI·ªÇN TH·ªä K·∫æT QU·∫¢ ---
        st.markdown("### K·∫øt qu·∫£ ph√¢n t√≠ch:")
        st.write(f"M√¥ h√¨nh ƒëang d√πng: **{model_choice}**")
        
        res_col1, res_col2 = st.columns(2)
        res_col1.metric("Nh√£n d·ª± ƒëo√°n", final_label)
        if final_conf:
            res_col2.metric("ƒê·ªô tin c·∫≠y", f"{final_conf:.2%}")
        else:
            res_col2.write("**ƒê·ªô tin c·∫≠y:** (Kh√¥ng h·ªó tr·ª£ tr√™n Linear SVM)")

# --- 6. PH·∫¶N TH·ªêNG K√ä (D∆∞·ªõi c√πng) ---
with st.expander("üìä Xem b·∫£ng ƒë·ªëi chi·∫øu th√¥ng s·ªë th·ª±c t·∫ø t·ª´ qu√° tr√¨nh hu·∫•n luy·ªán"):
    st.write("B·∫£ng s·ªë li·ªáu d·ª±a tr√™n k·∫øt qu·∫£ ki·ªÉm th·ª≠ tr√™n t·∫≠p Validation:")
    st.table({
        "Ti√™u ch√≠": ["Accuracy (ƒê·ªô ch√≠nh x√°c)", "F1-Macro (ƒê·ªô c√¢n b·∫±ng)", "∆Øu ƒëi·ªÉm"],
        "PhoBERT": ["78.36%", "0.6663", "Hi·ªÉu ng·ªØ c·∫£nh s√¢u"],
        "Logistic": ["78.18%", "0.6400", "·ªîn ƒë·ªãnh, t·ªëc ƒë·ªô nhanh"],
        "Linear SVM": ["78.63%", "0.6200", "Accuracy t·ªïng th·ªÉ cao"]
    })